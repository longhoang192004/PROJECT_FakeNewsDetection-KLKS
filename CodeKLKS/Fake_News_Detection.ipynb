{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DATA PROCESSING (TIỀN XỬ LÝ DỮ LIỆU)"
      ],
      "metadata": {
        "id": "_xPmacSruj2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import List, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "hgt14ZoRum6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHẦN 1: CÁC HÀM TIỀN XỬ LÝ\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_unicode(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return unicodedata.normalize('NFC', text)\n",
        "\n",
        "\n",
        "def minimal_clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # 1. Loại bỏ URLs (không mang thông tin ngữ nghĩa)\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' [URL] ', text)\n",
        "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' [URL] ', text)\n",
        "\n",
        "    # 2. Loại bỏ email (thông tin liên hệ, không phải nội dung)\n",
        "    text = re.sub(r'\\S+@\\S+', ' [EMAIL] ', text)\n",
        "\n",
        "    # 3. Loại bỏ số điện thoại\n",
        "    text = re.sub(r'(\\+84|0)[0-9]{1,3}[-.\\s]?[0-9]{3,4}[-.\\s]?[0-9]{3,4}', ' [PHONE] ', text)\n",
        "\n",
        "    # 4. Chuẩn hóa khoảng trắng (chỉ loại bỏ khoảng trắng THỪA)\n",
        "    # GIỮ LẠI multiple spaces nếu có ý nghĩa, nhưng thường thì không cần\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # 5. Loại bỏ khoảng trắng đầu cuối\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def preserve_features_clean(text: str) -> str:\n",
        "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "\n",
        "    # Unicode normalization\n",
        "    text = normalize_unicode(text)\n",
        "\n",
        "    # Minimal cleaning\n",
        "    text = minimal_clean_text(text)\n",
        "\n",
        "    # Final unicode normalization\n",
        "    text = normalize_unicode(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_statistics(df: pd.DataFrame, text_column: str = 'NỘI DUNG') -> Dict:\n",
        "    text_lengths = df[text_column].str.len()\n",
        "    word_counts = df[text_column].str.split().str.len()\n",
        "\n",
        "    stats = {\n",
        "        'char_length': {\n",
        "            'mean': text_lengths.mean(),\n",
        "            'median': text_lengths.median(),\n",
        "            'std': text_lengths.std(),\n",
        "            'min': text_lengths.min(),\n",
        "            'max': text_lengths.max(),\n",
        "            'percentiles': {\n",
        "                '50%': text_lengths.quantile(0.50),\n",
        "                '75%': text_lengths.quantile(0.75),\n",
        "                '90%': text_lengths.quantile(0.90),\n",
        "                '95%': text_lengths.quantile(0.95),\n",
        "                '99%': text_lengths.quantile(0.99),\n",
        "            }\n",
        "        },\n",
        "        'word_count': {\n",
        "            'mean': word_counts.mean(),\n",
        "            'median': word_counts.median(),\n",
        "            'min': word_counts.min(),\n",
        "            'max': word_counts.max(),\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "\n",
        "def analyze_fake_news_features(df: pd.DataFrame, text_col: str, label_col: str) -> Dict:\n",
        "    fake_df = df[df[label_col] == 0]\n",
        "    real_df = df[df[label_col] == 1]\n",
        "\n",
        "    def count_feature(text_series, pattern):\n",
        "        return text_series.apply(lambda x: len(re.findall(pattern, str(x)))).mean()\n",
        "\n",
        "    def uppercase_ratio(text_series):\n",
        "        def calc_ratio(text):\n",
        "            if not isinstance(text, str) or len(text) == 0:\n",
        "                return 0\n",
        "            uppercase = sum(1 for c in text if c.isupper())\n",
        "            letters = sum(1 for c in text if c.isalpha())\n",
        "            return uppercase / letters if letters > 0 else 0\n",
        "        return text_series.apply(calc_ratio).mean()\n",
        "\n",
        "    emoji_pattern = r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]'\n",
        "\n",
        "    features = {\n",
        "        'fake_news': {\n",
        "            'avg_exclamation': count_feature(fake_df[text_col], r'!'),\n",
        "            'avg_question': count_feature(fake_df[text_col], r'\\?'),\n",
        "            'avg_emoji': count_feature(fake_df[text_col], emoji_pattern),\n",
        "            'uppercase_ratio': uppercase_ratio(fake_df[text_col]),\n",
        "            'avg_numbers': count_feature(fake_df[text_col], r'\\d'),\n",
        "            'avg_length': fake_df[text_col].str.len().mean()\n",
        "        },\n",
        "        'real_news': {\n",
        "            'avg_exclamation': count_feature(real_df[text_col], r'!'),\n",
        "            'avg_question': count_feature(real_df[text_col], r'\\?'),\n",
        "            'avg_emoji': count_feature(real_df[text_col], emoji_pattern),\n",
        "            'uppercase_ratio': uppercase_ratio(real_df[text_col]),\n",
        "            'avg_numbers': count_feature(real_df[text_col], r'\\d'),\n",
        "            'avg_length': real_df[text_col].str.len().mean()\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def check_data_balance(df: pd.DataFrame, label_column: str = 'GIẢ(0)/THẬT(1)') -> Dict:\n",
        "    label_counts = df[label_column].value_counts()\n",
        "    total = len(df)\n",
        "\n",
        "    balance_info = {\n",
        "        'total_samples': total,\n",
        "        'class_distribution': label_counts.to_dict(),\n",
        "        'class_ratios': (label_counts / total * 100).to_dict(),\n",
        "        'imbalance_ratio': max(label_counts) / min(label_counts) if len(label_counts) > 1 else 1.0\n",
        "    }\n",
        "\n",
        "    return balance_info\n"
      ],
      "metadata": {
        "id": "jRR30-hquupN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHẦN 2: PIPELINE TIỀN XỬ LÝ\n",
        "# ============================================================================\n",
        "\n",
        "def process_dataset_minimal(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    text_column: str = 'NỘI DUNG',\n",
        "    label_column: str = 'GIẢ(0)/THẬT(1)',\n",
        "    verbose: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "\n",
        "    # 1. Đọc dữ liệu\n",
        "    df = pd.read_csv(input_path)\n",
        "    initial_size = len(df)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n THỐNG KÊ BAN ĐẦU:\")\n",
        "        print(f\"   Số mẫu: {initial_size}\")\n",
        "\n",
        "    # 2. Xử lý missing values\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "\n",
        "    # 3. Loại bỏ duplicates\n",
        "    df = df.drop_duplicates(subset=[text_column], keep='first')\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   Sau khi loại duplicates: {len(df)} mẫu\")\n",
        "\n",
        "    # 4. PHÂN TÍCH ĐẶC TRƯNG TRƯỚC KHI XỬ LÝ\n",
        "    if verbose:\n",
        "        print(f\"\\n PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (trước xử lý):\")\n",
        "        features = analyze_fake_news_features(df, text_column, label_column)\n",
        "\n",
        "        print(f\"\\n   FAKE NEWS:\")\n",
        "        print(f\"   - Trung bình dấu '!': {features['fake_news']['avg_exclamation']:.2f}\")\n",
        "        print(f\"   - Trung bình dấu '?': {features['fake_news']['avg_question']:.2f}\")\n",
        "        print(f\"   - Trung bình emoji: {features['fake_news']['avg_emoji']:.2f}\")\n",
        "        print(f\"   - Tỷ lệ CHỮ HOA: {features['fake_news']['uppercase_ratio']*100:.2f}%\")\n",
        "        print(f\"   - Trung bình số: {features['fake_news']['avg_numbers']:.2f}\")\n",
        "\n",
        "        print(f\"\\n   REAL NEWS:\")\n",
        "        print(f\"   - Trung bình dấu '!': {features['real_news']['avg_exclamation']:.2f}\")\n",
        "        print(f\"   - Trung bình dấu '?': {features['real_news']['avg_question']:.2f}\")\n",
        "        print(f\"   - Trung bình emoji: {features['real_news']['avg_emoji']:.2f}\")\n",
        "        print(f\"   - Tỷ lệ CHỮ HOA: {features['real_news']['uppercase_ratio']*100:.2f}%\")\n",
        "        print(f\"   - Trung bình số: {features['real_news']['avg_numbers']:.2f}\")\n",
        "\n",
        "    # 5. Áp dụng làm sạch TỐI THIỂU\n",
        "    if verbose:\n",
        "        print(f\"\")\n",
        "\n",
        "    df['processed_text'] = df[text_column].apply(preserve_features_clean)\n",
        "\n",
        "    # 6. Loại bỏ văn bản quá ngắn (< 5 ký tự) - threshold thấp hơn\n",
        "    df = df[df['processed_text'].str.len() >= 5]\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # 7. PHÂN TÍCH ĐẶC TRƯNG SAU KHI XỬ LÝ\n",
        "    if verbose:\n",
        "        print(f\"\\n PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (sau xử lý):\")\n",
        "        features_after = analyze_fake_news_features(df, 'processed_text', label_column)\n",
        "\n",
        "        print(f\"\\n   FAKE NEWS:\")\n",
        "        print(f\"   - Trung bình dấu '!': {features_after['fake_news']['avg_exclamation']:.2f}\")\n",
        "        print(f\"   - Trung bình dấu '?': {features_after['fake_news']['avg_question']:.2f}\")\n",
        "        print(f\"   - Trung bình emoji: {features_after['fake_news']['avg_emoji']:.2f}\")\n",
        "        print(f\"   - Tỷ lệ CHỮ HOA: {features_after['fake_news']['uppercase_ratio']*100:.2f}%\")\n",
        "\n",
        "        # So sánh trước vs sau\n",
        "        emoji_preserved = (features_after['fake_news']['avg_emoji'] / max(features['fake_news']['avg_emoji'], 0.001)) * 100\n",
        "        caps_preserved = (features_after['fake_news']['uppercase_ratio'] / max(features['fake_news']['uppercase_ratio'], 0.001)) * 100\n",
        "\n",
        "        print(f\"\\n    TỶ LỆ GIỮ LẠI ĐẶC TRƯNG:\")\n",
        "        print(f\"   - Emoji: {emoji_preserved:.1f}%\")\n",
        "        print(f\"   - Chữ HOA: {caps_preserved:.1f}%\")\n",
        "\n",
        "    # 8. Thống kê văn bản\n",
        "    stats = get_text_statistics(df, 'processed_text')\n",
        "    if verbose:\n",
        "        print(f\"\\n THỐNG KÊ VĂN BẢN:\")\n",
        "        print(f\"   - Độ dài trung bình: {stats['char_length']['mean']:.1f} ký tự\")\n",
        "        print(f\"   - Số từ trung bình: {stats['word_count']['mean']:.1f} từ\")\n",
        "        print(f\"   - Percentile 95%: {stats['char_length']['percentiles']['95%']:.0f} ký tự\")\n",
        "\n",
        "    # 9. Kiểm tra balance\n",
        "    balance_info = check_data_balance(df, label_column)\n",
        "    if verbose:\n",
        "        print(f\"\\n  PHÂN BỐ NHÃN:\")\n",
        "        for label, count in balance_info['class_distribution'].items():\n",
        "            ratio = balance_info['class_ratios'][label]\n",
        "            label_name = \"THẬT\" if label == 1 else \"GIẢ\"\n",
        "            print(f\"   - {label_name} ({label}): {count} ({ratio:.1f}%)\")\n",
        "\n",
        "    # 10. Tạo output DataFrame\n",
        "    output_df = pd.DataFrame({\n",
        "        'text': df['processed_text'],\n",
        "        'label': df[label_column],\n",
        "        'original_text': df[text_column]\n",
        "    })\n",
        "\n",
        "    # 11. Lưu file\n",
        "    output_df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n ĐÃ LƯU: {output_path}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return output_df"
      ],
      "metadata": {
        "id": "dGUPdZpEu2QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PHẦN 3: HÀM MAIN\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA PREPROCESSING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Định nghĩa đường dẫn\n",
        "    data_dir = '/content/sample_data'\n",
        "    output_dir = '/content/processed_data_minimal'\n",
        "\n",
        "    # Tạo thư mục output\n",
        "    import os\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    datasets = {\n",
        "        'train': {\n",
        "            'input': f'{data_dir}/train.csv',\n",
        "            'output': f'{output_dir}/train_processed.csv'\n",
        "        },\n",
        "        'val': {\n",
        "            'input': f'{data_dir}/val.csv',\n",
        "            'output': f'{output_dir}/val_processed.csv'\n",
        "        },\n",
        "        'test': {\n",
        "            'input': f'{data_dir}/test.csv',\n",
        "            'output': f'{output_dir}/test_processed.csv'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Xử lý từng dataset\n",
        "    processed_dfs = {}\n",
        "\n",
        "    for dataset_name, paths in datasets.items():\n",
        "        try:\n",
        "            df = process_dataset_minimal(\n",
        "                input_path=paths['input'],\n",
        "                output_path=paths['output'],\n",
        "                verbose=True\n",
        "            )\n",
        "            processed_dfs[dataset_name] = df\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"\\n Không tìm thấy: {paths['input']}\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"\\n Lỗi {dataset_name}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Tổng kết\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TỔNG KẾT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for dataset_name, df in processed_dfs.items():\n",
        "        print(f\"\\n{dataset_name.upper()}: {len(df)} mẫu\")\n",
        "        print(f\"   → {datasets[dataset_name]['output']}\")\n",
        "\n",
        "    # Khuyến nghị\n",
        "    if 'train' in processed_dfs:\n",
        "        stats = get_text_statistics(processed_dfs['train'], 'text')\n",
        "        max_len = int(stats['char_length']['percentiles']['95%'])\n",
        "\n",
        "    print(\"HOÀN THÀNH!\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    return processed_dfs\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processed_data = main()\n",
        "\n",
        "    # Hiển thị mẫu\n",
        "    if 'train' in processed_data:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MẪU DỮ LIỆU (3 dòng đầu):\")\n",
        "        print(\"=\"*80)\n",
        "        sample = processed_data['train'][['text', 'label']].head(3)\n",
        "        for idx, row in sample.iterrows():\n",
        "            print(f\"\\n[{idx+1}] Label: {row['label']} ({'THẬT' if row['label']==1 else 'GIẢ'})\")\n",
        "            print(f\"Text: {row['text'][:200]}...\")\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KffbGfDlu5Sa",
        "outputId": "11b20ad4-21c4-47bf-806c-914cac6182c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DATA PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "\n",
            " THỐNG KÊ BAN ĐẦU:\n",
            "   Số mẫu: 361\n",
            "   Sau khi loại duplicates: 220 mẫu\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (trước xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.02\n",
            "   - Trung bình dấu '?': 0.24\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.63%\n",
            "   - Trung bình số: 31.14\n",
            "\n",
            "   REAL NEWS:\n",
            "   - Trung bình dấu '!': 1.60\n",
            "   - Trung bình dấu '?': 1.64\n",
            "   - Trung bình emoji: 0.26\n",
            "   - Tỷ lệ CHỮ HOA: 3.91%\n",
            "   - Trung bình số: 14.32\n",
            "\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (sau xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.02\n",
            "   - Trung bình dấu '?': 0.24\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.64%\n",
            "\n",
            "    TỶ LỆ GIỮ LẠI ĐẶC TRƯNG:\n",
            "   - Emoji: 0.0%\n",
            "   - Chữ HOA: 100.1%\n",
            "\n",
            " THỐNG KÊ VĂN BẢN:\n",
            "   - Độ dài trung bình: 2784.7 ký tự\n",
            "   - Số từ trung bình: 623.1 từ\n",
            "   - Percentile 95%: 6342 ký tự\n",
            "\n",
            "  PHÂN BỐ NHÃN:\n",
            "   - GIẢ (0): 122 (55.5%)\n",
            "   - THẬT (1): 98 (44.5%)\n",
            "\n",
            " ĐÃ LƯU: /content/processed_data_minimal/train_processed.csv\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            " THỐNG KÊ BAN ĐẦU:\n",
            "   Số mẫu: 45\n",
            "   Sau khi loại duplicates: 43 mẫu\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (trước xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.04\n",
            "   - Trung bình dấu '?': 0.21\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.68%\n",
            "   - Trung bình số: 31.46\n",
            "\n",
            "   REAL NEWS:\n",
            "   - Trung bình dấu '!': 1.00\n",
            "   - Trung bình dấu '?': 1.21\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 4.57%\n",
            "   - Trung bình số: 21.00\n",
            "\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (sau xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.04\n",
            "   - Trung bình dấu '?': 0.21\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.69%\n",
            "\n",
            "    TỶ LỆ GIỮ LẠI ĐẶC TRƯNG:\n",
            "   - Emoji: 0.0%\n",
            "   - Chữ HOA: 100.3%\n",
            "\n",
            " THỐNG KÊ VĂN BẢN:\n",
            "   - Độ dài trung bình: 2495.6 ký tự\n",
            "   - Số từ trung bình: 556.9 từ\n",
            "   - Percentile 95%: 5670 ký tự\n",
            "\n",
            "  PHÂN BỐ NHÃN:\n",
            "   - GIẢ (0): 24 (55.8%)\n",
            "   - THẬT (1): 19 (44.2%)\n",
            "\n",
            " ĐÃ LƯU: /content/processed_data_minimal/val_processed.csv\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            " THỐNG KÊ BAN ĐẦU:\n",
            "   Số mẫu: 46\n",
            "   Sau khi loại duplicates: 43 mẫu\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (trước xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.04\n",
            "   - Trung bình dấu '?': 0.54\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.14%\n",
            "   - Trung bình số: 31.58\n",
            "\n",
            "   REAL NEWS:\n",
            "   - Trung bình dấu '!': 0.89\n",
            "   - Trung bình dấu '?': 0.58\n",
            "   - Trung bình emoji: 0.05\n",
            "   - Tỷ lệ CHỮ HOA: 2.75%\n",
            "   - Trung bình số: 12.63\n",
            "\n",
            "\n",
            " PHÂN TÍCH ĐẶC TRƯNG FAKE NEWS (sau xử lý):\n",
            "\n",
            "   FAKE NEWS:\n",
            "   - Trung bình dấu '!': 0.04\n",
            "   - Trung bình dấu '?': 0.54\n",
            "   - Trung bình emoji: 0.00\n",
            "   - Tỷ lệ CHỮ HOA: 5.14%\n",
            "\n",
            "    TỶ LỆ GIỮ LẠI ĐẶC TRƯNG:\n",
            "   - Emoji: 0.0%\n",
            "   - Chữ HOA: 100.0%\n",
            "\n",
            " THỐNG KÊ VĂN BẢN:\n",
            "   - Độ dài trung bình: 10672.9 ký tự\n",
            "   - Số từ trung bình: 2409.0 từ\n",
            "   - Percentile 95%: 5658 ký tự\n",
            "\n",
            "  PHÂN BỐ NHÃN:\n",
            "   - GIẢ (0): 24 (55.8%)\n",
            "   - THẬT (1): 19 (44.2%)\n",
            "\n",
            " ĐÃ LƯU: /content/processed_data_minimal/test_processed.csv\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "TỔNG KẾT\n",
            "================================================================================\n",
            "\n",
            "TRAIN: 220 mẫu\n",
            "   → /content/processed_data_minimal/train_processed.csv\n",
            "\n",
            "VAL: 43 mẫu\n",
            "   → /content/processed_data_minimal/val_processed.csv\n",
            "\n",
            "TEST: 43 mẫu\n",
            "   → /content/processed_data_minimal/test_processed.csv\n",
            "HOÀN THÀNH!\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "MẪU DỮ LIỆU (3 dòng đầu):\n",
            "================================================================================\n",
            "\n",
            "[1] Label: 1 (THẬT)\n",
            "Text: Lý do Phi Nhung phải uống thuốc trừ sâu tự tử khiến ai cũng ‘choáng’? Phi Nhung lần đầu tiết lộ về bí mật ‘sốc’ rằng, cô từng uống thuốc trừ sâu tự tử khiến ai cũng choáng vì bất ngờ. Trên trang fanpa...\n",
            "\n",
            "[2] Label: 1 (THẬT)\n",
            "Text: Vén quần thấy giun bò lúc nhúc trong bắp chân, tới bệnh viện mới ói mửa khi biết sự thật bên trong Cứ ngỡ gân xanh nổi lên khắp bắp chân, nhưng không ngờ thứ dây dưa dài ngoằng đục khoét bên trong mới...\n",
            "\n",
            "[3] Label: 1 (THẬT)\n",
            "Text: Bức vẽ giúp bạn đánh giá mức độ stress của bản thân Nhìn bức tranh, bạn có thể thấy vòng tròn đang chuyển động một cách chậm rãi. Người khác lại cho biết họ thấy hình ảnh chuyển động rất nhanh và đau ...\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}